<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>Ask AI — Browser-Only</title>
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <style>
    :root { font-family: system-ui, Arial, sans-serif; }
    body { margin: 0; padding: 24px; background:#0b1020; color:#e8eefc; }
    .wrap { max-width: 840px; margin: 0 auto; }
    h1 { margin: 0 0 12px; font-size: 1.25rem; opacity:.9 }
    form { display:flex; gap:8px; margin: 12px 0 16px; }
    input[type="text"] { flex:1; padding:12px; border-radius:12px; border:1px solid #2a3358; background:#0f1733; color:#e8eefc; }
    button { padding:12px 16px; border-radius:12px; border:0; background:#5b8cff; color:#0b1020; font-weight:700; cursor:pointer; }
    .status { font-size:.9rem; opacity:.8; margin: 6px 0 12px; }
    .card { background:#0f1733; border:1px solid #2a3358; border-radius:16px; padding:16px; }
    .answer { white-space: pre-wrap; line-height:1.45 }
    .spinner { display:inline-block; width:1em; height:1em; border:.18em solid #5b8cff; border-top-color:transparent; border-radius:50%; animation:spin 1s linear infinite; vertical-align:-.2em }
    @keyframes spin { to { transform:rotate(360deg) } }
    .small { font-size:.85rem; opacity:.75 }
    .muted { opacity:.7 }
  </style>
</head>
<body>
  <div class="wrap">
    <h1>Ask AI</h1>
    <div class="small muted">Runs fully in your browser using a tiny open model. First load may take a bit while the model downloads and caches.</div>

    <form id="ask-form">
      <input id="q" type="text" placeholder="Ask a question…" autocomplete="off" />
      <button id="submit" type="submit">Ask</button>
    </form>

    <div class="status" id="status">Loading model… <span class="spinner"></span></div>

    <div class="card">
      <div class="answer" id="answer">Ask something to get started.</div>
    </div>
  </div>

  <!-- WebLLM via unpkg -->
  <script src="https://unpkg.com/@mlc-ai/web-llm/dist/webllm.min.js"></script>
  <script>
    const statusEl = document.getElementById('status');
    const answerEl = document.getElementById('answer');
    const form = document.getElementById('ask-form');
    const input = document.getElementById('q');
    const submitBtn = document.getElementById('submit');

    // Choose a small instruct model that runs in-browser. 
    // Good starters: "Llama-3.2-1B-Instruct-q4f16_1" or "Phi-2-q4f16_1".
    const MODEL_NAME = "Llama-3.2-1B-Instruct-q4f16_1";

    let engine;
    let ready = false;

    (async () => {
      try {
        statusEl.textContent = "Initializing WebGPU + model…";
        engine = await webllm.CreateMLCEngine({
          model: MODEL_NAME,
          // You can tweak cache or low-ram configs here if needed:
          // cache: "indexeddb",
          // context_window_size: 2048,
        }, {
          initProgressCallback: (report) => {
            const { progress, text } = report;
            statusEl.textContent = `Model setup: ${text} ${progress ? Math.round(progress*100) + "%" : ""}`;
          }
        });
        ready = true;
        statusEl.textContent = `Ready: ${MODEL_NAME} loaded.`;
      } catch (e) {
        console.error(e);
        statusEl.textContent = "Failed to initialize WebLLM. Try a modern desktop browser with WebGPU.";
      }
    })();

    form.addEventListener('submit', async (e) => {
      e.preventDefault();
      if (!ready) return;
      const q = input.value.trim();
      if (!q) return;

      submitBtn.disabled = true;
      input.disabled = true;
      answerEl.textContent = "";
      statusEl.innerHTML = `Thinking… <span class="spinner"></span>`;

      // Simple system instruction to keep it concise and helpful
      const messages = [
        { role: "system", content: "You are a helpful, concise assistant." },
        { role: "user", content: q }
      ];

      try {
        // Stream tokens into the page
        const stream = await engine.chat.completions.create({
          messages,
          stream: true,
          temperature: 0.7,
          max_tokens: 512
        });

        for await (const chunk of stream) {
          const delta = chunk.choices?.[0]?.delta?.content ?? "";
          if (delta) answerEl.textContent += delta;
        }
        statusEl.textContent = "Done.";
      } catch (err) {
        console.error(err);
        statusEl.textContent = "Error generating a response.";
      } finally {
        submitBtn.disabled = false;
        input.disabled = false;
        input.select();
      }
    });
  </script>
</body>
</html>